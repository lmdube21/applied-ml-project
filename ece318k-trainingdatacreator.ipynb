{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T14:26:29.938776Z","iopub.status.busy":"2023-12-01T14:26:29.938265Z","iopub.status.idle":"2023-12-01T14:26:30.391729Z","shell.execute_reply":"2023-12-01T14:26:30.390479Z","shell.execute_reply.started":"2023-12-01T14:26:29.938737Z"},"id":"_WsucU-k_QtG","outputId":"366e71ce-9ddb-4744-dd37-81d64b22602c","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","import re\n","import pickle\n","from functools import reduce"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T14:26:32.181376Z","iopub.status.busy":"2023-12-01T14:26:32.180847Z","iopub.status.idle":"2023-12-01T14:26:48.995397Z","shell.execute_reply":"2023-12-01T14:26:48.994152Z","shell.execute_reply.started":"2023-12-01T14:26:32.181333Z"},"trusted":true},"outputs":[],"source":["raw_training_data = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n","raw_training_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T14:27:06.091764Z","iopub.status.busy":"2023-12-01T14:27:06.091365Z","iopub.status.idle":"2023-12-01T14:27:06.119608Z","shell.execute_reply":"2023-12-01T14:27:06.118457Z","shell.execute_reply.started":"2023-12-01T14:27:06.091732Z"},"trusted":true},"outputs":[],"source":["raw_training_data.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:20:21.823343Z","iopub.status.busy":"2023-11-29T17:20:21.822948Z","iopub.status.idle":"2023-11-29T17:20:21.834090Z","shell.execute_reply":"2023-11-29T17:20:21.832974Z","shell.execute_reply.started":"2023-11-29T17:20:21.823315Z"},"trusted":true},"outputs":[],"source":["raw_training_data = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T18:30:44.416265Z","iopub.status.busy":"2023-11-30T18:30:44.415859Z","iopub.status.idle":"2023-11-30T18:30:44.740085Z","shell.execute_reply":"2023-11-30T18:30:44.738690Z","shell.execute_reply.started":"2023-11-30T18:30:44.416204Z"},"trusted":true},"outputs":[],"source":["raw_training_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Essay Recreator (credit given in comment at top)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#####CODE FROM: https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor\n","\n","def getEssays(df):\n","    # Copy required columns\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n","    \n","    # Get rid of text inputs that make no change\n","    # Note: Shift was unpreditcable so ignored\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","\n","    # Get how much each Id there is\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","\n","    # Holds the final index of the previous Id\n","    lastIndex = 0\n","\n","    # Holds all the essays\n","    essaySeries = pd.Series()\n","\n","    # Fills essay series with essays\n","    for index, valCount in enumerate(valCountsArr):\n","\n","        # Indexes down_time at current Id\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","\n","        # Update the last index\n","        lastIndex += valCount\n","\n","        # Where the essay content will be stored\n","        essayText = \"\"\n","\n","        \n","        # Produces the essay\n","        for Input in currTextInput.values:\n","            \n","            # Input[0] = activity\n","            # Input[2] = cursor_position\n","            # Input[3] = text_change\n","            \n","            # If activity = Replace\n","            if Input[0] == 'Replace':\n","                # splits text_change at ' => '\n","                replaceTxt = Input[2].split(' => ')\n","                \n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","\n","                \n","            # If activity = Paste    \n","            if Input[0] == 'Paste':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Remove/Cut\n","            if Input[0] == 'Remove/Cut':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Move...\n","            if \"M\" in Input[0]:\n","                # Gets rid of the \"Move from to\" text\n","                croppedTxt = Input[0][10:]\n","                \n","                # Splits cropped text by ' To '\n","                splitTxt = croppedTxt.split(' To ')\n","                \n","                # Splits split text again by ', ' for each item\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                \n","                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n","                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","\n","                # Skip if someone manages to activiate this by moving to same place\n","                if moveData[0] != moveData[2]:\n","                    # Check if they move text forward in essay (they are different)\n","                    if moveData[0] < moveData[2]:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","                \n","                \n","            # If just input\n","            # DONT TOUCH\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","\n","            \n","        # Sets essay at index  \n","        essaySeries[index] = essayText\n","     \n","    \n","    # Sets essay series index to the ids\n","    essaySeries.index =  textInputDf['id'].unique()\n","    \n","    \n","    # Returns the essay series\n","    return essaySeries"]},{"cell_type":"markdown","metadata":{},"source":["Fuctions to Create Features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:20:27.499801Z","iopub.status.busy":"2023-11-29T17:20:27.499362Z","iopub.status.idle":"2023-11-29T17:20:27.517280Z","shell.execute_reply":"2023-11-29T17:20:27.516085Z","shell.execute_reply.started":"2023-11-29T17:20:27.499767Z"},"trusted":true},"outputs":[],"source":["\n","\n","def scale_within_group(group):\n","        min_value = group['cursor_position'].min()\n","        max_value = group['cursor_position'].max()\n","        group['cursor_position'] = group['cursor_position'] - min_value\n","        return group\n","    \n","def segment_df_creator(df, num_segments):\n","\n","        segment_dict = {}\n","        last_action_time = 1860000\n","        interval_len = int(round(last_action_time/num_segments, 0))\n","\n","\n","        for i in range(num_segments):\n","            if i == 0: \n","                segment_dict[str(i)] = df[df['up_time'] < interval_len].reset_index()\n","\n","\n","            elif i == max(range(num_segments)):\n","                segment_dict[str(i)] = df[df['up_time'] >interval_len*max(range(num_segments))].reset_index()\n","\n","            else: \n","                segment_dict[str(i)] = df[(df['up_time']>interval_len*(i-1)) & (df['up_time']<interval_len*(i)) ] .reset_index()\n","\n","        return segment_dict\n","    \n","def column_list(df, addendum):\n","    cols = list(df.columns)\n","    new_cols = []\n","    for col in cols:\n","        if col != 'id':\n","            new_cols.append(f'{col}_{addendum}')\n","        else: \n","            new_cols.append(col)\n","    return new_cols\n","        \n","\n","def get_revision_ratio(df):\n","    # Filter rows where 'activity' is either \"Remove/Cut\" or \"Input\"\n","    filtered_df = df[df['activity'].isin([\"Remove/Cut\", \"Input\"])]\n","\n","    # Calculate revise count and activity count for respective conditions\n","    revise_count = filtered_df.loc[filtered_df['activity'] == \"Remove/Cut\", 'text_change'].str.len().sum()\n","    activity_count = filtered_df.loc[filtered_df['activity'] == \"Input\", 'text_change'].str.len().sum()\n","\n","    # Calculate revision ratio\n","    if revise_count + activity_count == 0:\n","        return 0\n","    else: \n","        return revise_count / (revise_count + activity_count)\n","\n","\n","def punctuation_ratios(df):\n","    \n","    punct_dict = {}\n","    try:\n","        comma_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \",\")])\n","        comma_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \",\")])\n","        punct_dict['comma_total'] = comma_count-comma_minus\n","    except: punct_dict['comma_total'] = 0\n","    try: \n","        period_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \".\")])\n","        period_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \".\")])\n","        punct_dict['period_total'] = period_count-period_minus\n","        \n","    except: punct_dict['period_total'] = 0\n","    try: \n","        dash_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \"-\")])\n","        dash_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \"-\")])\n","        punct_dict['dash_total'] = dash_count-dash_minus\n","    except: punct_dict['dash_total'] = 0\n","    try:\n","        qm_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \"?\")])\n","        qm_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \"?\")])\n","        punct_dict['qm_total'] =  qm_count-qm_minus\n","        \n","    except: punct_dict['qm_total'] = 0\n","    try:\n","        ep_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \"!\")])\n","        ep_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \"!\")])\n","        punct_dict['ep_total'] =  ep_count-ep_minus\n","    except: punct_dict['ep_total'] = 0\n","    try: \n","        sc_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \";\")])\n","        sc_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \";\")])\n","        punct_dict['sc_total'] =  sc_count-sc_minus\n","    except: punct_dict['sc_total'] = 0\n","    try: \n","        colon_count = len(df[(df['activity'] == \"Input\") &(df['text_change'] == \":\")])\n","        colon_minus = len(df[(df['activity'] == \"Remove/Cut\") &(df['text_change'] == \":\")])\n","        punct_dict['colon_total'] =  colon_count-colon_minus\n","    except: punct_dict['colon_total'] = 0\n","     \n","    try:\n","        ratio_dict = {}\n","        sumpunct = 0\n","        for key2 in punct_dict.keys():\n","                sumpunct += punct_dict[key2]\n","        for key in punct_dict.keys():\n","            ratio_dict[key.split('_')[0]] = punct_dict[key]/sumpunct\n","\n","        return ratio_dict\n","    except: \n","        for key in punct_dict.keys():\n","            ratio_dict[key.split('_')[0]] = 0\n","        return  ratio_dict\n","    \n","    \n","def sentence_length_stats(essay):\n","    try: \n","        sentences = re.split(r'[.!?]', essay)\n","        # Remove empty segments if any\n","        sentences = [sentence for sentence in sentences if sentence]\n","        word_counts = []\n","        for sentence in sentences: \n","            word_counts.append(sentence.count(' '))\n","        mean = np.mean(word_counts)\n","        sd = np.std(word_counts)\n","        return mean, sd\n","    except: \n","        return 0,0\n","\n","def word_len_stats(essay):\n","    try:\n","        words = re.findall(r'\\w+|\\S', essay)\n","        words = [word for word in words if word]\n","\n","        # Count the number of 'q's in each word to represent the word length\n","        word_lengths = [word.count('q') for word in words if word.isalpha()]\n","\n","        mean = sum(word_lengths) / len(word_lengths)\n","\n","        # Calculate the standard deviation of word length\n","        sd = np.std(word_lengths)\n","    \n","        return mean, sd\n","    \n","    except: \n","        return 0,0\n","    \n","def create_sum_stats(df):\n","    sum_df = df.groupby('id', as_index=False).agg(\n","    max_word_count=('word_count', 'max'), \n","     min_word_count=('word_count', 'min'),\n","     median_word_count = ('word_count', 'median'),\n","     mean_word_count = ('word_count', 'mean'),\n","     std_word_count = ('word_count', 'std'), \n","     skew_word_count = ('word_count', 'skew'),\n","     max_action_time=('action_time', 'max'), \n","     min_action_time=('action_time', 'min'),\n","     median_action_time = ('action_time', 'median'),\n","     mean_action_time = ('action_time', 'mean'),\n","     std_action_time = ('action_time', 'std'), \n","     skew_action_time = ('action_time', 'skew'),\n","     max_inter_action_time=('inter_action_time', 'max'), \n","     min_inter_action_time=('inter_action_time', 'min'),\n","     median_inter_action_time = ('inter_action_time', 'median'),\n","     mean_inter_action_time = ('inter_action_time', 'mean'),\n","     std_inter_action_time = ('inter_action_time', 'std'), \n","     skew_inter_action_time = ('inter_action_time', 'skew'))\n","    return sum_df.reset_index()\n","\n","def essay_output_to_df(series):\n","    essay_summary_df = series.to_frame(name='essay')\n","    essay_summary_df['id'] = essay_summary_df.index\n","    essay_summary_df.reset_index(inplace = True)\n","    del(essay_summary_df['index'])\n","    essay_summary_df['mean_word_len'] = essay_summary_df['essay'].apply(lambda x: word_len_stats(x)[0])\n","    essay_summary_df['std_word_len'] = essay_summary_df['essay'].apply(lambda x: word_len_stats(x)[1])\n","    essay_summary_df['mean_sentence_len'] = essay_summary_df['essay'].apply(lambda x: sentence_length_stats(x)[0])\n","    essay_summary_df['std_sentence_len'] = essay_summary_df['essay'].apply(lambda x: sentence_length_stats(x)[0])\n","    del(essay_summary_df['essay'])\n","    return essay_summary_df\n","\n","def calc_output_to_df(df):\n","    punct_results = df.groupby('id').apply(punctuation_ratios).reset_index()\n","    punct_summary_df = pd.DataFrame(punct_results.iloc[:, 1].tolist())\n","    punct_summary_df['id'] = punct_results['id']\n","\n","    revision_results = df.groupby('id').apply(get_revision_ratio).reset_index()\n","    revision_results.columns = ['id', 'revision_ratio']\n","    \n","    total_rows_per_id = df.groupby('id').size().reset_index()\n","    total_rows_per_id.columns = ['id', 'total_actions']\n","    \n","    one_min_pauses = df[df['inter_action_time'] > 60000].groupby('id')['inter_action_time'].count().reset_index()\n","    one_min_pauses.columns = ['id', 'one_min_pause_count']\n","    one_min = pd.merge(one_min_pauses,total_rows_per_id, how = 'left', on = 'id')\n","    one_min['one_min_pause_to_output_ratio'] = 0  # Initialize ratio column with zeros\n","    one_min.loc[one_min['total_actions'] != 0, 'one_min_pause_to_output_ratio'] = one_min['one_min_pause_count'] / one_min['total_actions']\n","    del(one_min['total_actions'])\n","\n","    ten_sec_pauses = df[df['inter_action_time'] > 10000].groupby('id')['inter_action_time'].count().reset_index()\n","    ten_sec_pauses.columns = ['id', 'ten_sec_pause_count']\n","    ten_sec = pd.merge(ten_sec_pauses,total_rows_per_id, how = 'left', on = 'id')\n","    ten_sec['ten_sec_pause_to_output_ratio'] = 0  # Initialize ratio column with zeros\n","    ten_sec.loc[ten_sec['total_actions'] != 0, 'ten_sec_pause_to_output_ratio'] = ten_sec['ten_sec_pause_count'] / ten_sec['total_actions']\n","    del(ten_sec['total_actions'])\n","    \n","    calc_sum_df = pd.merge(punct_summary_df, revision_results, how='left', on = 'id')\n","    calc_sum_df = pd.merge(calc_sum_df, total_rows_per_id, how = 'left', on = 'id')\n","    calc_sum_df = pd.merge(calc_sum_df, one_min, how='left', on = 'id')\n","    calc_sum_df = pd.merge(calc_sum_df, ten_sec, how='left', on = 'id')\n","    return calc_sum_df\n","\n","def data_processing(df):\n","    filtered_df = raw_training_data.dropna()\n","\n","    #removing data outside of the time threshold plus one minute\n","    filtered_df = filtered_df.groupby('id').filter(lambda x: (x['up_time'] <= 1860000).all())\n","    #Creating inter_action_variable\n","    filtered_df.sort_values(by=['id', 'up_time'], inplace=True)\n","    filtered_df['inter_action_time'] = filtered_df.groupby('id')['up_time'].diff()\n","    num_segments = 6\n","\n","    \n","\n","    segmented_data_dict = segment_df_creator(filtered_df, num_segments)\n","\n","    \n","\n","    for key in segmented_data_dict.keys():\n","        segmented_data_dict[key] = segmented_data_dict[key].groupby('id', as_index = False).apply(scale_within_group)\n","        segmented_data_dict[key]['cursor_position'] = segmented_data_dict[key]['cursor_position'].astype(int)\n","        segmented_data_dict[key]['actions_occured'] = 1\n","    segmented_data_dict['full'] = filtered_df\n","    \n","    essay_dict = {\n","    \"0\":None,\n","    \"1\":None,\n","    \"2\":None,\n","    \"3\":None,\n","    \"4\":None,\n","    \"5\":None,\n","    \"full\":None\n","    }\n","    for key in essay_dict.keys():\n","\n","        essay_dict[key] = getEssays(segmented_data_dict[key])\n","        \n","    summary_columns = ['total_actions', 'total_words', 'periods_count', 'q_count', 'space_count']\n","\n","    summary_df = create_sum_stats(filtered_df)\n","    summary_df_0_5 = create_sum_stats(segmented_data_dict[\"0\"])\n","    summary_df_0_5.columns = column_list(summary_df_0_5,'0_5')\n","    summary_df_5_10 = create_sum_stats(segmented_data_dict[\"1\"])\n","    summary_df_5_10.columns = column_list(summary_df_5_10,'5_10')\n","    summary_df_10_15 = create_sum_stats(segmented_data_dict[\"2\"])\n","    summary_df_10_15.columns = column_list(summary_df_10_15,'10_15')\n","    summary_df_15_20 = create_sum_stats(segmented_data_dict[\"3\"])\n","    summary_df_15_20.columns = column_list(summary_df_15_20,'15_20')\n","    summary_df_20_25 = create_sum_stats(segmented_data_dict[\"4\"])\n","    summary_df_20_25.columns = column_list(summary_df_20_25,'20_25')\n","    summary_df_25_30 = create_sum_stats(segmented_data_dict[\"5\"])\n","    summary_df_25_30.columns = column_list(summary_df_25_30,'25_30')\n","\n","    # List of DataFrames\n","    summary_df_list = [summary_df, summary_df_0_5, summary_df_5_10,summary_df_10_15, summary_df_15_20,  summary_df_20_25, summary_df_25_30 ]\n","\n","    # Merge DataFrames on 'key_column'\n","    full_summary_data_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), summary_df_list)\n","\n","    essay_df = essay_output_to_df(essay_dict['full'])\n","    essay_df_0_5 = essay_output_to_df(essay_dict['0'])\n","    essay_df_0_5.columns = column_list(essay_df_0_5,'0_5')\n","    essay_df_5_10 = essay_output_to_df(essay_dict['1'])\n","    essay_df_5_10.columns = column_list(essay_df_5_10,'5_10')\n","    essay_df_10_15 = essay_output_to_df(essay_dict['2'])\n","    essay_df_10_15.columns = column_list(essay_df_10_15,'10_15')\n","    essay_df_15_20 = essay_output_to_df(essay_dict['3'])\n","    essay_df_15_20.columns = column_list(essay_df_15_20,'15_20')\n","    essay_df_20_25 = essay_output_to_df(essay_dict['4'])\n","    essay_df_20_25.columns = column_list(essay_df_20_25,'20_25')\n","    essay_df_25_30 = essay_output_to_df(essay_dict['5'])\n","    essay_df_25_30.columns = column_list(essay_df_25_30,'25_30')\n","\n","    essay_df_list = [essay_df, essay_df_0_5, essay_df_5_10,essay_df_10_15, essay_df_15_20,  essay_df_20_25, essay_df_25_30 ]\n","\n","    # Merge DataFrames on 'key_column'\n","    full_essay_data_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), essay_df_list)\n","\n","    calc_stats_df = calc_output_to_df(segmented_data_dict['full'])\n","    calc_stats_df_0_5 = calc_output_to_df(segmented_data_dict['0'])\n","    calc_stats_df_0_5.columns = column_list(calc_stats_df_0_5,'0_5')\n","    calc_stats_df_5_10 = calc_output_to_df(segmented_data_dict['1'])\n","    calc_stats_df_5_10.columns = column_list(calc_stats_df_5_10,'5_10')\n","    calc_stats_df_10_15 = calc_output_to_df(segmented_data_dict['2'])\n","    calc_stats_df_10_15.columns = column_list(calc_stats_df_10_15,'10_15')\n","    calc_stats_df_15_20 = calc_output_to_df(segmented_data_dict['3'])\n","    calc_stats_df_15_20.columns = column_list(calc_stats_df_15_20,'15_20')\n","    calc_stats_df_20_25 = calc_output_to_df(segmented_data_dict['4'])\n","    calc_stats_df_20_25.columns = column_list(calc_stats_df_20_25,'20_25')\n","    calc_stats_df_25_30 = calc_output_to_df(segmented_data_dict['5'])\n","    calc_stats_df_25_30.columns = column_list(calc_stats_df_25_30,'25_30')\n","\n","\n","    calc_df_list = [calc_stats_df, calc_stats_df_0_5, calc_stats_df_5_10,calc_stats_df_10_15, calc_stats_df_15_20,  calc_stats_df_20_25, calc_stats_df_25_30 ]\n","\n","    # Merge DataFrames on 'key_column'\n","    full_calc_data_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), calc_df_list)\n","\n","    \n","    all_df_list = [full_calc_data_df, full_essay_data_df, full_summary_data_df, raw_training_scores ]\n","\n","    # Merge DataFrames on 'key_column'\n","    training_dataset_feature_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), all_df_list)\n"]},{"cell_type":"markdown","metadata":{},"source":["Some Initial Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:16:26.431000Z","iopub.status.busy":"2023-11-29T17:16:26.430320Z","iopub.status.idle":"2023-11-29T17:16:26.482639Z","shell.execute_reply":"2023-11-29T17:16:26.481534Z","shell.execute_reply.started":"2023-11-29T17:16:26.430969Z"},"trusted":true},"outputs":[],"source":["\n","nan_counts = raw_training_data.isna().sum()\n","print(nan_counts)\n","nan_rows = raw_training_data[raw_training_data.isnull().any(axis=1)]\n","nan_rows\n","\n","\n","raw_training_data.columns #columns included in the data\n","\n","len(raw_training_data)#number of data points\n","\n","raw_training_data.head(15)\n","len((raw_training_data['id']).unique()) #number of unique essays\n","\n","#information on the number of words per essay\n","# Group the DataFrame by 'id' and get the last row of each group\n","last_rows = raw_training_data.groupby('id').last()\n","\n","# Extract the 'word count' column from the last rows\n","word_counts = last_rows['word_count']\n","print(max(word_counts))\n","print(min(word_counts))\n","\n","print(word_counts.mean())\n","print(word_counts.std())\n","\n","\n","#information on the numer of actions per essay\n","value_counts = raw_training_data['id'].value_counts()\n","\n","# Calculate the average number of occurrences\n","average_count = value_counts.mean()\n","sd_counts = value_counts.std()\n","print(average_count)\n","print(sd_counts)\n","print(max(value_counts))\n","print(min(value_counts))\n","\n","raw_training_data['activity'].value_counts()\n","\n","minvals = raw_training_data.groupby('id').min()\n","maxvals = raw_training_data.groupby('id').max()\n","\n","min(maxvals['up_time'])\n","\n","print(len(raw_training_data['id'].unique()))\n","filtered_df = raw_training_data.groupby('id').filter(lambda x: (x['up_time'] <= 1860000).all())\n","print(len(filtered_df['id'].unique()))\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Data Filtering"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:16:31.356727Z","iopub.status.busy":"2023-11-29T17:16:31.356032Z","iopub.status.idle":"2023-11-29T17:16:31.374306Z","shell.execute_reply":"2023-11-29T17:16:31.372689Z","shell.execute_reply.started":"2023-11-29T17:16:31.356682Z"},"trusted":true},"outputs":[],"source":["#removing_na_columns\n","filtered_df = raw_training_data.dropna()\n","\n","#removing data outside of the time threshold plus one minute\n","filtered_df = filtered_df.groupby('id').filter(lambda x: (x['up_time'] <= 1860000).all())\n","#Creating inter_action_variable\n","filtered_df.sort_values(by=['id', 'up_time'], inplace=True)\n","filtered_df['inter_action_time'] = filtered_df.groupby('id')['up_time'].diff()\n","\n","\n","#setting all events without at least 10000 instances to \"other\"\n"]},{"cell_type":"markdown","metadata":{},"source":["Creation of Essay Sections"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:16:35.620744Z","iopub.status.busy":"2023-11-29T17:16:35.620352Z","iopub.status.idle":"2023-11-29T17:16:35.670210Z","shell.execute_reply":"2023-11-29T17:16:35.669075Z","shell.execute_reply.started":"2023-11-29T17:16:35.620715Z"},"trusted":true},"outputs":[],"source":["%%time\n","num_segments = 6\n","\n","def segment_df_creator(df, num_segments):\n","\n","    segment_dict = {}\n","    last_action_time = 1860000\n","    interval_len = int(round(last_action_time/num_segments, 0))\n","    \n","    \n","    for i in range(num_segments):\n","        if i == 0: \n","            segment_dict[str(i)] = df[df['up_time'] < interval_len].reset_index()\n","        \n","            \n","        elif i == max(range(num_segments)):\n","            segment_dict[str(i)] = df[df['up_time'] >interval_len*max(range(num_segments))].reset_index()\n","        \n","        else: \n","            segment_dict[str(i)] = df[(df['up_time']>interval_len*(i-1)) & (df['up_time']<interval_len*(i)) ] .reset_index()\n","        \n","    return segment_dict\n","\n","segmented_data_dict = segment_df_creator(filtered_df, num_segments)\n","\n","def scale_within_group(group):\n","    min_value = group['cursor_position'].min()\n","    max_value = group['cursor_position'].max()\n","    group['cursor_position'] = group['cursor_position'] - min_value\n","    return group\n","\n","for key in segmented_data_dict.keys():\n","    segmented_data_dict[key] = segmented_data_dict[key].groupby('id', as_index = False).apply(scale_within_group)\n","    segmented_data_dict[key]['cursor_position'] = segmented_data_dict[key]['cursor_position'].astype(int)\n","    segmented_data_dict[key]['actions_occured'] = 1\n","segmented_data_dict['full'] = filtered_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:19:56.808457Z","iopub.status.busy":"2023-11-29T17:19:56.808030Z","iopub.status.idle":"2023-11-29T17:19:56.815629Z","shell.execute_reply":"2023-11-29T17:19:56.814342Z","shell.execute_reply.started":"2023-11-29T17:19:56.808424Z"},"trusted":true},"outputs":[],"source":["len(raw_training_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:19:17.579481Z","iopub.status.busy":"2023-11-29T17:19:17.579036Z","iopub.status.idle":"2023-11-29T17:19:17.588909Z","shell.execute_reply":"2023-11-29T17:19:17.587764Z","shell.execute_reply.started":"2023-11-29T17:19:17.579450Z"},"trusted":true},"outputs":[],"source":["print(len(segmented_data_dict['0']['id'].unique()))\n","print(len(segmented_data_dict['1']['id'].unique()))\n","print(len(segmented_data_dict['2']['id'].unique()))\n","print(len(segmented_data_dict['3']['id'].unique()))\n","print(len(segmented_data_dict['4']['id'].unique()))\n","print(len(segmented_data_dict['5']['id'].unique()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T04:22:30.944371Z","iopub.status.busy":"2023-11-24T04:22:30.943660Z","iopub.status.idle":"2023-11-24T04:22:32.361244Z","shell.execute_reply":"2023-11-24T04:22:32.359814Z","shell.execute_reply.started":"2023-11-24T04:22:30.944336Z"},"trusted":true},"outputs":[],"source":["display(segmented_data_dict['full'][segmented_data_dict['full']['id'] == '0144e4d5'])"]},{"cell_type":"markdown","metadata":{},"source":["Addition of Function to Write Out Essays for Easy Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:16:52.394599Z","iopub.status.busy":"2023-11-29T17:16:52.394202Z","iopub.status.idle":"2023-11-29T17:16:52.416436Z","shell.execute_reply":"2023-11-29T17:16:52.415370Z","shell.execute_reply.started":"2023-11-29T17:16:52.394550Z"},"trusted":true},"outputs":[],"source":["#####CODE FROM: https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor\n","\n","def getEssays(df):\n","    # Copy required columns\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n","    \n","    # Get rid of text inputs that make no change\n","    # Note: Shift was unpreditcable so ignored\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","\n","    # Get how much each Id there is\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","\n","    # Holds the final index of the previous Id\n","    lastIndex = 0\n","\n","    # Holds all the essays\n","    essaySeries = pd.Series()\n","\n","    # Fills essay series with essays\n","    for index, valCount in enumerate(valCountsArr):\n","\n","        # Indexes down_time at current Id\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","\n","        # Update the last index\n","        lastIndex += valCount\n","\n","        # Where the essay content will be stored\n","        essayText = \"\"\n","\n","        \n","        # Produces the essay\n","        for Input in currTextInput.values:\n","            \n","            # Input[0] = activity\n","            # Input[2] = cursor_position\n","            # Input[3] = text_change\n","            \n","            # If activity = Replace\n","            if Input[0] == 'Replace':\n","                # splits text_change at ' => '\n","                replaceTxt = Input[2].split(' => ')\n","                \n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","\n","                \n","            # If activity = Paste    \n","            if Input[0] == 'Paste':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Remove/Cut\n","            if Input[0] == 'Remove/Cut':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Move...\n","            if \"M\" in Input[0]:\n","                # Gets rid of the \"Move from to\" text\n","                croppedTxt = Input[0][10:]\n","                \n","                # Splits cropped text by ' To '\n","                splitTxt = croppedTxt.split(' To ')\n","                \n","                # Splits split text again by ', ' for each item\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                \n","                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n","                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","\n","                # Skip if someone manages to activiate this by moving to same place\n","                if moveData[0] != moveData[2]:\n","                    # Check if they move text forward in essay (they are different)\n","                    if moveData[0] < moveData[2]:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","                \n","                \n","            # If just input\n","            # DONT TOUCH\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","\n","            \n","        # Sets essay at index  \n","        essaySeries[index] = essayText\n","     \n","    \n","    # Sets essay series index to the ids\n","    essaySeries.index =  textInputDf['id'].unique()\n","    \n","    \n","    # Returns the essay series\n","    return essaySeries"]},{"cell_type":"markdown","metadata":{},"source":["Writing out of essays - by section and total"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:17:03.581826Z","iopub.status.busy":"2023-11-29T17:17:03.581356Z","iopub.status.idle":"2023-11-29T17:17:03.620663Z","shell.execute_reply":"2023-11-29T17:17:03.619752Z","shell.execute_reply.started":"2023-11-29T17:17:03.581790Z"},"trusted":true},"outputs":[],"source":["\n","#THE OUTPUT IS LOADED IN NEXT CELL\n","\n","essay_dict = {\n","    \"0\":None,\n","    \"1\":None,\n","    \"2\":None,\n","    \"3\":None,\n","    \"4\":None,\n","    \"5\":None,\n","    \"full\":None\n","}\n","for key in essay_dict.keys():\n","    \n","    essay_dict[key] = getEssays(segmented_data_dict[key])\n"]},{"cell_type":"markdown","metadata":{},"source":["Essay-Formatted Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T04:22:32.409825Z","iopub.status.busy":"2023-11-24T04:22:32.409162Z","iopub.status.idle":"2023-11-24T04:22:32.422513Z","shell.execute_reply":"2023-11-24T04:22:32.421269Z","shell.execute_reply.started":"2023-11-24T04:22:32.409791Z"},"trusted":true},"outputs":[],"source":["#Saving the data\n","\"\"\"\n","with open('essays.pickle', 'wb') as handle:\n","    pickle.dump(essay_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["Loading the Essay formatted Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T20:14:09.348703Z","iopub.status.busy":"2023-11-28T20:14:09.347571Z","iopub.status.idle":"2023-11-28T20:14:09.380942Z","shell.execute_reply":"2023-11-28T20:14:09.379897Z","shell.execute_reply.started":"2023-11-28T20:14:09.348653Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/essay-segments-written-form2/essays.pickle', 'rb') as handle:\n","    essay_dict = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T20:14:43.918974Z","iopub.status.busy":"2023-11-28T20:14:43.918536Z","iopub.status.idle":"2023-11-28T20:14:43.930985Z","shell.execute_reply":"2023-11-28T20:14:43.929556Z","shell.execute_reply.started":"2023-11-28T20:14:43.918939Z"},"trusted":true},"outputs":[],"source":["display(essay_dict['1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T20:14:12.263636Z","iopub.status.busy":"2023-11-28T20:14:12.263265Z","iopub.status.idle":"2023-11-28T20:14:12.372999Z","shell.execute_reply":"2023-11-28T20:14:12.371257Z","shell.execute_reply.started":"2023-11-28T20:14:12.263600Z"},"trusted":true},"outputs":[],"source":["print(len(essay_dict['0']['id'].unique()))\n","print(len(essay_dict['1']['id'].unique()))\n","print(len(essay_dict['2']['id'].unique()))\n","print(len(essay_dict['3']['id'].unique()))\n","print(len(essay_dict['4']['id'].unique()))\n","print(len(essay_dict['5']['id'].unique()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:17:31.209481Z","iopub.status.busy":"2023-11-29T17:17:31.209090Z","iopub.status.idle":"2023-11-29T17:17:31.237100Z","shell.execute_reply":"2023-11-29T17:17:31.235839Z","shell.execute_reply.started":"2023-11-29T17:17:31.209450Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["SUMMARY INFORMATION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:17:46.661778Z","iopub.status.busy":"2023-11-29T17:17:46.661336Z","iopub.status.idle":"2023-11-29T17:17:46.942516Z","shell.execute_reply":"2023-11-29T17:17:46.941130Z","shell.execute_reply.started":"2023-11-29T17:17:46.661727Z"},"id":"IRTgOS5hi1W6","outputId":"8050360c-3891-46ec-e2e7-cdab04687719","trusted":true},"outputs":[],"source":["summary_columns = ['total_actions', 'total_words', 'periods_count', 'q_count', 'space_count']\n","\n","     \n","        \n","\n","    \n","summary_df = create_sum_stats(filtered_df)\n","summary_df_0_5 = create_sum_stats(segmented_data_dict[\"0\"])\n","summary_df_0_5.columns = column_list(summary_df_0_5,'0_5')\n","summary_df_5_10 = create_sum_stats(segmented_data_dict[\"1\"])\n","summary_df_5_10.columns = column_list(summary_df_5_10,'5_10')\n","summary_df_10_15 = create_sum_stats(segmented_data_dict[\"2\"])\n","summary_df_10_15.columns = column_list(summary_df_10_15,'10_15')\n","summary_df_15_20 = create_sum_stats(segmented_data_dict[\"3\"])\n","summary_df_15_20.columns = column_list(summary_df_15_20,'15_20')\n","summary_df_20_25 = create_sum_stats(segmented_data_dict[\"4\"])\n","summary_df_20_25.columns = column_list(summary_df_20_25,'20_25')\n","summary_df_25_30 = create_sum_stats(segmented_data_dict[\"5\"])\n","summary_df_25_30.columns = column_list(summary_df_25_30,'25_30')\n","\n","# List of DataFrames\n","summary_df_list = [summary_df, summary_df_0_5, summary_df_5_10,summary_df_10_15, summary_df_15_20,  summary_df_20_25, summary_df_25_30 ]\n","\n","# Merge DataFrames on 'key_column'\n","full_summary_data_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), summary_df_list)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Creation of Functions to Calculate Desired Features Based on Literature"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:18:31.965271Z","iopub.status.busy":"2023-11-29T17:18:31.964826Z","iopub.status.idle":"2023-11-29T17:18:32.031268Z","shell.execute_reply":"2023-11-29T17:18:32.029990Z","shell.execute_reply.started":"2023-11-29T17:18:31.965236Z"},"trusted":true},"outputs":[],"source":["\n","essay_df = essay_output_to_df(essay_dict['full'])\n","essay_df_0_5 = essay_output_to_df(essay_dict['0'])\n","essay_df_0_5.columns = column_list(essay_df_0_5,'0_5')\n","essay_df_5_10 = essay_output_to_df(essay_dict['1'])\n","essay_df_5_10.columns = column_list(essay_df_5_10,'5_10')\n","essay_df_10_15 = essay_output_to_df(essay_dict['2'])\n","essay_df_10_15.columns = column_list(essay_df_10_15,'10_15')\n","essay_df_15_20 = essay_output_to_df(essay_dict['3'])\n","essay_df_15_20.columns = column_list(essay_df_15_20,'15_20')\n","essay_df_20_25 = essay_output_to_df(essay_dict['4'])\n","essay_df_20_25.columns = column_list(essay_df_20_25,'20_25')\n","essay_df_25_30 = essay_output_to_df(essay_dict['5'])\n","essay_df_25_30.columns = column_list(essay_df_25_30,'25_30')\n","\n","essay_df_list = [essay_df, essay_df_0_5, essay_df_5_10,essay_df_10_15, essay_df_15_20,  essay_df_20_25, essay_df_25_30 ]\n","\n","# Merge DataFrames on 'key_column'\n","full_essay_data_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), essay_df_list)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T17:18:35.429685Z","iopub.status.busy":"2023-11-29T17:18:35.428984Z","iopub.status.idle":"2023-11-29T17:18:35.798176Z","shell.execute_reply":"2023-11-29T17:18:35.796371Z","shell.execute_reply.started":"2023-11-29T17:18:35.429649Z"},"trusted":true},"outputs":[],"source":["\n","\n","calc_stats_df = calc_output_to_df(segmented_data_dict['full'])\n","calc_stats_df_0_5 = calc_output_to_df(segmented_data_dict['0'])\n","calc_stats_df_0_5.columns = column_list(calc_stats_df_0_5,'0_5')\n","calc_stats_df_5_10 = calc_output_to_df(segmented_data_dict['1'])\n","calc_stats_df_5_10.columns = column_list(calc_stats_df_5_10,'5_10')\n","calc_stats_df_10_15 = calc_output_to_df(segmented_data_dict['2'])\n","calc_stats_df_10_15.columns = column_list(calc_stats_df_10_15,'10_15')\n","calc_stats_df_15_20 = calc_output_to_df(segmented_data_dict['3'])\n","calc_stats_df_15_20.columns = column_list(calc_stats_df_15_20,'15_20')\n","calc_stats_df_20_25 = calc_output_to_df(segmented_data_dict['4'])\n","calc_stats_df_20_25.columns = column_list(calc_stats_df_20_25,'20_25')\n","calc_stats_df_25_30 = calc_output_to_df(segmented_data_dict['5'])\n","calc_stats_df_25_30.columns = column_list(calc_stats_df_25_30,'25_30')\n","\n","\n","calc_df_list = [calc_stats_df, calc_stats_df_0_5, calc_stats_df_5_10,calc_stats_df_10_15, calc_stats_df_15_20,  calc_stats_df_20_25, calc_stats_df_25_30 ]\n","\n","# Merge DataFrames on 'key_column'\n","full_calc_data_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), calc_df_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T04:28:16.636960Z","iopub.status.busy":"2023-11-24T04:28:16.636503Z","iopub.status.idle":"2023-11-24T04:28:18.111885Z","shell.execute_reply":"2023-11-24T04:28:18.110491Z","shell.execute_reply.started":"2023-11-24T04:28:16.636921Z"},"trusted":true},"outputs":[],"source":["\n","\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.max_rows', 500)\n","df = segmented_data_dict['full'][segmented_data_dict['full']['id'] == '0178a105']\n","total_rows_per_id = df.groupby('id').size().reset_index()\n","total_rows_per_id.columns = ['id', 'total_actions']\n","display(full_calc_data_df[full_calc_data_df['id'] == '0178a105'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T04:28:18.114595Z","iopub.status.busy":"2023-11-24T04:28:18.113790Z","iopub.status.idle":"2023-11-24T04:28:18.153038Z","shell.execute_reply":"2023-11-24T04:28:18.151782Z","shell.execute_reply.started":"2023-11-24T04:28:18.114550Z"},"trusted":true},"outputs":[],"source":["all_df_list = [full_calc_data_df, full_essay_data_df, full_summary_data_df, raw_training_scores ]\n","\n","# Merge DataFrames on 'key_column'\n","training_dataset_feature_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), all_df_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T04:28:18.155363Z","iopub.status.busy":"2023-11-24T04:28:18.154572Z","iopub.status.idle":"2023-11-24T04:28:18.174877Z","shell.execute_reply":"2023-11-24T04:28:18.173795Z","shell.execute_reply.started":"2023-11-24T04:28:18.155332Z"},"trusted":true},"outputs":[],"source":["for col in training_dataset_feature_df.columns:\n","    \n","    if 'total_actions' in col:\n","        training_dataset_feature_df[col] = training_dataset_feature_df[col].fillna(0)\n","    elif 'min_pause' in col: \n","        training_dataset_feature_df[col] = training_dataset_feature_df[col].fillna(0)\n","    elif 'sec_pause' in col: \n","        training_dataset_feature_df[col] = training_dataset_feature_df[col].fillna(0)\n","    else: pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T20:45:22.383689Z","iopub.status.busy":"2023-11-28T20:45:22.383309Z","iopub.status.idle":"2023-11-28T20:45:23.057871Z","shell.execute_reply":"2023-11-28T20:45:23.056753Z","shell.execute_reply.started":"2023-11-28T20:45:22.383659Z"},"trusted":true},"outputs":[],"source":["\n","\n","training_data['action_occured_0_5'] = np.where(training_data['id'].isin(segmented_data_dict['0']['id'].unique()), 1, 0)\n","    \n","training_data['action_occured_5_10'] = np.where(training_data['id'].isin(segmented_data_dict['1']['id'].unique()), 1, 0)\n","\n","training_data['action_occured_10_15'] = np.where(training_data['id'].isin(segmented_data_dict['2']['id'].unique()), 1, 0)\n","\n","training_data['action_occured_15_20'] = np.where(training_data['id'].isin(segmented_data_dict['3']['id'].unique()), 1, 0)\n","\n","training_data['action_occured_20_25'] = np.where(training_data['id'].isin(segmented_data_dict['4']['id'].unique()), 1, 0)\n","\n","training_data['action_occured_25_30'] = np.where(training_data['id'].isin(segmented_data_dict['5']['id'].unique()), 1, 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T20:46:40.049935Z","iopub.status.busy":"2023-11-28T20:46:40.049536Z","iopub.status.idle":"2023-11-28T20:46:40.059186Z","shell.execute_reply":"2023-11-28T20:46:40.057980Z","shell.execute_reply.started":"2023-11-28T20:46:40.049904Z"},"trusted":true},"outputs":[],"source":["training_data['action_occured_0_5'].value_counts()\n","training_data['action_occured_20_25'].value_counts()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T18:06:18.958892Z","iopub.status.busy":"2023-11-29T18:06:18.958482Z","iopub.status.idle":"2023-11-29T18:06:22.188878Z","shell.execute_reply":"2023-11-29T18:06:22.187460Z","shell.execute_reply.started":"2023-11-29T18:06:18.958862Z"},"trusted":true},"outputs":[],"source":["from sklearn.impute import KNNImputer\n","import numpy as np\n","data = training_data.copy()\n","del(data['id'])\n","\n","# Specify the index of the column to be ignored during imputation\n","  # Change this to the column name you want to ignore\n","\n","# Copy the DataFrame for imputation\n","data_copy = data.copy()\n","\n","# Convert the column to ignore to NumPy for imputation\n","column_index =0\n","data_array = data_copy.values\n","\n","\n","# Perform imputation\n","imputer = KNNImputer(n_neighbors=2, weights='distance', metric='nan_euclidean')\n","imputed_data_array = imputer.fit_transform(data_array)\n","\n","\n","\n","# Convert back to Pandas DataFrame\n","columns = list(data_copy.columns)\n","\n","final_df = pd.DataFrame(imputed_data_array, columns=columns)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T18:06:37.398680Z","iopub.status.busy":"2023-11-29T18:06:37.398274Z","iopub.status.idle":"2023-11-29T18:06:37.404426Z","shell.execute_reply":"2023-11-29T18:06:37.403587Z","shell.execute_reply.started":"2023-11-29T18:06:37.398649Z"},"trusted":true},"outputs":[],"source":["final_df['id'] = training_data['id']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T18:07:14.151265Z","iopub.status.busy":"2023-11-29T18:07:14.149553Z","iopub.status.idle":"2023-11-29T18:07:14.163219Z","shell.execute_reply":"2023-11-29T18:07:14.162293Z","shell.execute_reply.started":"2023-11-29T18:07:14.151203Z"},"trusted":true},"outputs":[],"source":["len(final_df.dropna())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T18:08:33.315810Z","iopub.status.busy":"2023-11-29T18:08:33.315283Z","iopub.status.idle":"2023-11-29T18:08:34.276909Z","shell.execute_reply":"2023-11-29T18:08:34.275651Z","shell.execute_reply.started":"2023-11-29T18:08:33.315757Z"},"trusted":true},"outputs":[],"source":["final_df.to_csv('imputed_training_data.csv')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"},{"datasetId":4004681,"sourceId":6970125,"sourceType":"datasetVersion"},{"datasetId":4004761,"sourceId":6970289,"sourceType":"datasetVersion"},{"datasetId":4050440,"sourceId":7040194,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
